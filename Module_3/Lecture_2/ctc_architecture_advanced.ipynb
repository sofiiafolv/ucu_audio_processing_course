{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ASR Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Seq2Seq architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/whisper.png)\n",
    "Example of Seq2Seq architecture - Whisper model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On the left is the transformer encoder. This takes as input a log-mel spectrogram and encodes that spectrogram to form a sequence of encoder hidden states that extract important features from the spoken speech. This hidden-states tensor represents the input sequence as a whole and effectively encodes the “meaning” of the input speech.\n",
    "\n",
    "The output of the encoder is then passed into the transformer decoder, shown on the right, using a mechanism called cross-attention. This is like self-attention but attends over the encoder output. From this point on, the encoder is no longer needed.\n",
    "\n",
    "The decoder predicts a sequence of text tokens in an autoregressive manner, a single token at a time, starting from an initial sequence that just has a “start” token in it (SOT in the case of Whisper). At each following timestep, the previous output sequence is fed back into the decoder as the new input sequence. In this manner, the decoder emits one new token at a time, steadily growing the output sequence, until it predicts an “end” token or a maximum number of timesteps is reached.\n",
    "\n",
    "While the architecture of the decoder is mostly identical to that of the encoder, there are two big differences:\n",
    "1. the decoder has a cross-attention mechanism that allows it to look at the encoder’s representation of the input sequence\n",
    "2. the decoder’s attention is causal — the decoder isn’t allowed to look into the future.\n",
    "\n",
    "A typical loss function for a seq2seq ASR model is the cross-entropy loss, as the final layer of the model predicts a probability distribution over the possible tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CTC architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "CTC or Connectionist Temporal Classification is a technique that is used with encoder-only transformer models for automatic speech recognition. Examples of such models are Wav2Vec2, HuBERT and M-CTC-T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Wav2Vec2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/wav2vec2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Feature encoder: This is the encoder part of the model. It takes the raw audio data as input and outputs feature vectors. Input size is limited to 400 samples which is 20ms for 16kHz sample rate. The raw audio is first standardized to have zero mean and unit variance. Then it is passed to 1D convolutional neural network (temporal convolution) followed by layer normalization and GELU activation function. There could be 7 such convolution blocks with constant channel size (512), decreasing kernel width (10, 3x4, 2x2) and stride (5, 2x6). The output is list of feature vectors each with 512 dimensions.\n",
    "\n",
    "Transformers: The output of the feature encoder is passed on to a transformer layer. One differentiator is use of relative positional embedding by using convolution layers, rather than using fixed positional encoding as done in original Transformers paper. The block size differs, as 12 transformers block with model dimension of 768 is used in BASE model but 24 blocks with 1024 dimension in LARGE version.\n",
    "\n",
    "Quantization module: For self-supervised learning, we need to work with discrete outputs. For this, there is a quantization module that converts the continous vector output to discrete representations, and on top of it, it automatically learns the discrete speech units. This is done by maintaining multiple codebooks/groups (320 in size) and the units are sampled from each codebook are later concatenated (320x320=102400 possiblt speech units). The sampling is done using Gumbel-Softmax which is like argmax but differentiable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wav2Vec2 as pre-training + CTC as fine-tuning = ASR :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "An encoder-only transformer is the simplest kind of transformer because it just uses the encoder portion of the model. The encoder reads the input sequence (the audio waveform) and maps this into a sequence of hidden-states, also known as the output embeddings.\n",
    "\n",
    "With a CTC model, we apply an additional linear mapping on the sequence of hidden-states to get class label predictions. The class labels are the characters of the alphabet (a, b, c, …). This way we’re able to predict any word in the target language with a small classification head, as the vocabulary just needs to exist of 26 characters plus a few special tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/ctc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let’s suppose our input is a one-second audio file. In Wav2Vec2, the model first downsamples the audio input using the CNN feature encoder to a shorter sequence of hidden-states, where there is one hidden-state vector for every 20 milliseconds of audio. For one second of audio, we then forward a sequence of 50 hidden-states to the transformer encoder. (The audio segments extracted from the input sequence partially overlap, so even though one hidden-state vector is emitted every 20 ms, each hidden-state actually represent 25 ms of audio.)\n",
    "\n",
    "The transformer encoder predicts one feature representation for each of these hidden-states, meaning we receive a sequence of 50 outputs from the transformer. Each of these outputs has a dimensionality of 768. The output sequence of the transformer encoder in this example therefore has shape (768, 50). As each of these predictions covers 25 ms of time, which is shorter than the duration of a phoneme, it makes sense to predict individual phonemes or characters but not entire words. CTC works best with a small vocabulary, so we’ll predict characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/ctc1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To make text predictions, we map each of the 768-dimensional encoder outputs to our character labels using a linear layer (the “CTC head”). The model then predicts a (50, 32) tensor containing the logits, where 32 is the number of tokens in the vocabulary. Since we make one prediction for each of the features in the sequence, we end up with a total of 50 character predictions for each second of audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CTC (Connectionist Temporal Classification) - a decoding algorithm and loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input, and then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence.\n",
    "\n",
    "Let’s imagine inference on someone saying the word dinner, and let’s suppose we had a function that chooses the most probable letter for each input spectral frame representation $x_i$. We’ll call the sequence of letters corresponding to each input frame an alignment, because it tells us where in the acoustic signal each letter aligns to. Figure 1 shows one such alignment, and what happens if we use a collapsing function that just removes consecutive duplicate letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/basic_decoding.png)\n",
    "Figure 1: A naive algorithm for collapsing an alignment between input and letters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Well, that doesn’t work; our naive algorithm has transcribed the speech as diner, not dinner! Collapsing doesn’t handle double letters. There’s also another problem with our naive function; it doesn’t tell us what symbol to align with silence in the input. We don’t want to be transcribing silence as random letters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The key decoding problem\n",
    "1. We have a decode RRROODD\n",
    "2. Is it ROD or ROOD?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The CTC algorithm solves both problems by adding to the transcription alphabet a special symbol for a blank, which we’ll represent as ⌴. The blank can be used in the alignment whenever we don’t want to transcribe a letter. Blank can also be used between letters; since our collapsing function collapses only consecutive duplicate letters, it won’t collapse across ⌴. More formally, let’s define the mapping $B : a → y$ between an alignment $a$ and an output $y$, which collapses all repeated letters and then removes all blanks. Figure 2 sketches this collapsing function B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The symbol set that the network can recognise must include ⌴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/ctc_collapsing_function.png)\n",
    "Figure 2: The CTC collapsing function B, showing the space blank character ⌴; repeated (consecutive) characters in an alignment A are removed to form the output Y ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The CTC collapsing function is many-to-one; lots of different alignments map to the same output string. For example, the alignment shown in Figure 2 is not the only alignment that results in the string dinner. Figure 2 shows some other alignments that would produce the same output. We’ll use the inverse of our B function, called B<sup>-1</sup>, and represent that set as B<sup>-1</sup>(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/other_alignments.png)\n",
    "Figure 3: Three other legitimate alignments producing the transcript dinner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CTC Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before we see how to compute $P_{CTC}(Y|X)$, let's first see how CTC assigns a probability to one particular alignment $\\hat{A} = { \\hat{a}_1, \\ldots, \\hat{a}_n }$. CTC makes a strong conditional independence assumption: it assumes that, given the input $X$, the CTC model output $a_t$ at time $t$ is independent of the output labels at any other time $a_i$. Thus:\n",
    "$$P_{\\text{ctc}}(A|X) = \\prod_{t=1}^{T} p(a_t | X)$$\n",
    "\n",
    "Thus to find the best alignment $\\hat{A} = { \\hat{a}_1, \\ldots, \\hat{a}_n }$ we can greedily choose the character with the max probability at each time step t: $$a_t = \\text{argmax}_{c \\in C} p_t(c | X)$$\n",
    "\n",
    "We then pass the resulting sequence A to the CTC collapsing function B to get the output sequence Y.\n",
    "\n",
    "We implemented Best path decoding. Best path decoding is the simplest method to decode the output matrix:\n",
    "1. Concatenate most probable characters per time-step which yields the best path.\n",
    "2. Then, undo the encoding by first removing duplicate characters and then removing all blanks. This gives us the recognized text.\n",
    "\n",
    "In Figure 3 you might see the example of Best path decoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/most_likely_characters.png)\n",
    "Figure 3: Concatenate most probable characters per time-step to get best path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Why best path decoding can fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/all_paths_to_a.png)\n",
    "Figure 4: All paths corresponding to text \"a\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Best path decoding is both fast and simple, which are of course nice properties. But it may fail in certain situations like the one shown in Fig 2. In Fig. 3 all paths corresponding to the text “a” are shown: \"aa\", ‘a⌴’ and ‘⌴a’. The probability of the text \"a\" is the sum over all probabilities of these mentioned paths: 0.2·0.4+0.2·0.6+0.8·0.4=0.52. So, \"a\" is more probable than \"⌴\" (0.52>0.48). We need a better algorithm than best path decoding which can handle such situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For this reason, the most probable output sequence Y is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments:\n",
    "\n",
    "$$P_{\\text{CTC}}(Y|X) = \\sum_{A \\in B^{-1}(Y)} P(A|X) = \\sum_{A \\in B^{-1}(Y)} \\prod_{t=1}^{T} p(a_t|h_t)$$\n",
    "\n",
    "where $h_t$ is a hidden state of the encoder at timestep $t$.\n",
    "\n",
    "$$Y = \\text{argmax}_Y P_{\\text{CTC}}(Y|X)$$\n",
    "\n",
    "But summing over all alignments is very expensive (there are a lot of alignments)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"images/semi_tree.png\" alt=\"Image 1\" style=\"width: 100%;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<img src=\"images/decoding_graph.png\" alt=\"Image 2\" style=\"width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You have only 3 elements in your dictionary! In the real word you would have a huge amount of them. Beam search can help here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Image Title](images/beam_search.png)\n",
    "Figure 5: Visualization of the naive beam search\n",
    "\n",
    "If two beams are equal, we simply merge them: we add up the scores and only keep one of the beams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Because of the strong conditional independence assumption mentioned earlier (that the output at time t is independent of the output at time t − 1, given the input), CTC does not implicitly learn a language model over the data (unlike the attention-based encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model.\n",
    "\n",
    "A character-level language model (LM) scores a sequence of characters. We restrict our LM to score single characters (unigram LM) and pairs of characters (bigram LM). We denote a unigram probability of the character c as P(c) and the bigram probability of characters c1, c2 as P(c2|c1). The score of a text “hello” is the probability of seeing a single “h”, and the probability of seeing a pair “h” and “e” next to each other, and a pair “e” and “l” next to each other, …\n",
    "\n",
    "Training such a LM from a large text is easy: we simply count how often a character occurs and divide by the total number of characters to get the unigram probability. And we count how often a pair of characters occurs and normalize it to get the bigram probability.\n",
    "\n",
    " Just think of a language model as a function taking a sentence as input, which is often only partly constructed, and returning the probability of the last word given all the previous words. What is this good for? Imagine we have the following partly constructed sentence: “tell us a fairy”. Now we hear the next word, but are not able discern whether it should be “tale” or “tail”. These words are homophones, which means they sound alike, although they are spelled differently. A well trained language model should be able to tell us that “tale” is much more probable than “tail”, making the choice straight forward. For further background reading, you can check out the Wikipedia page on language models.\n",
    "\n",
    "The final formula to receive score of sequence:\n",
    "\n",
    "$$score_{\\text{CTC}}(Y|X) = \\log P_{\\text{CTC}}(Y|X) + \\lambda_1 \\log P_{\\text{LM}}(Y) + \\lambda_2 L(Y)$$\n",
    "\n",
    "TODO1: what does L(Y) mean?\n",
    "TODO2: do we need Language Model in Seq2Seq architecture? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CTC Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset D is the sum of the negative log-likelihoods of the correct output Y for each input X:\n",
    "\n",
    "$$L_{\\text{CTC}} = \\sum_{(X,Y) \\in D} -\\log P_{\\text{CTC}}(Y|X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# One more thing before training ASR - Text Normalization. Why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Example of Whisper text normalization:\n",
    "\n",
    "For English Language:\n",
    "1. Remove any phrases between matching brackets ([, ]).\n",
    "2. Remove any phrases between matching parentheses ((, )).\n",
    "3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n",
    "4. Remove whitespace characters that come before an apostrophe ’.\n",
    "5. Convert standard or informal contracted forms of English into the original form (don't -> do not.\n",
    "6. Remove commas (,) between digits.\n",
    "7. Remove periods (.) not followed by numbers (It's 9.30 a.m. Are you free... -> It's 9.30 a.m Are you free...).\n",
    "8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category starting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n",
    "9. Detect any numeric expressions of numbers and currencies and replace them with a form using Arabic numbers, e.g., “Ten thousand dollars” → “$10000”.\n",
    "10. Convert British spellings into American spellings.\n",
    "11. Remove remaining symbols that are not part of any numeric expressions.\n",
    "12. Replace any successive whitespace characters with a space.\n",
    "\n",
    "For Other Languages:\n",
    "1. Remove any phrases between matching brackets ([, ]).\n",
    "2. Remove any phrases between matching parentheses ((, )).\n",
    "3. Replace any markers, symbols, and punctuation characters with a space, i.e., when the Unicode category of each character in the NFKC-normalized string starts with M, S, or P.\n",
    "4. Make the text lowercase.\n",
    "5. Replace any successive whitespace characters with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
